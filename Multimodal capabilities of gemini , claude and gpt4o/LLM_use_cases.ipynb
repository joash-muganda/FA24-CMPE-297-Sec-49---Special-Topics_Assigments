{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT**"
      ],
      "metadata": {
        "id": "tb-sx2yp29xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text samarization**"
      ],
      "metadata": {
        "id": "NvFhOc-p3Fvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install openai wikipedia-api\n",
        "\n",
        "import openai\n",
        "import wikipediaapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "whWPhcJF5G6R",
        "outputId": "95a98399-472d-44e6-864b-39ada7dbed75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.43.0)\n",
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#API Key\n",
        "client = openai.OpenAI(api_key = \"sk-bT_gSHSavznUjdStdUo3S07x4lWSIBo2xTdUn7QmPjT3BlbkFJvO8OwNrKPMPV5BizO3mZkafQh6qYzAqpJfCT5nAEAA\")"
      ],
      "metadata": {
        "id": "bzUMx2Nk5dme"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "    # Initialize the Wikipedia API with a custom user agent\n",
        "    wiki_wiki = wikipediaapi.Wikipedia(\n",
        "        language='en',\n",
        "        user_agent='ColabDemoScript/1.0 (Contact: myemail@example.com)'\n",
        "    )\n",
        "\n",
        "    # Fetch the page\n",
        "    page = wiki_wiki.page(page_title)\n",
        "\n",
        "    # Check if the page exists\n",
        "    if page.exists():\n",
        "        return page.text\n",
        "    else:\n",
        "        print(f\"Page '{page_title}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "def summarize_text(input_text):\n",
        "    # Use the client to create a chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n{input_text}\"}\n",
        "        ],\n",
        "        max_tokens=100  # Adjust max_tokens as needed\n",
        "    )\n",
        "    # Use the model's attributes to access the content\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "\n",
        "# Specify the Wikipedia page title you want to summarize\n",
        "page_title = \"Artificial intelligence\"  # Replace with any Wikipedia page title\n",
        "\n",
        "# Fetch the content from the Wikipedia page\n",
        "wiki_content = get_wikipedia_content(page_title)\n",
        "\n",
        "if wiki_content:\n",
        "    # Summarize the fetched content\n",
        "    summary = summarize_text(wiki_content[:2000])  # Limiting the length for summarization\n",
        "    print(\"Summary:\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67MzKaI63DIR",
        "outputId": "f92427b7-90ef-44de-c178-ca7d8e0ca2a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " Artificial intelligence (AI) refers to the intelligence displayed by machines, especially computer systems. It's a field of research within computer science that designs and studies methods and software enabling machines to understand their environment and act intelligently to achieve specific goals. Examples of AI applications include web search engines, recommendation systems, voice interaction systems, autonomous vehicles, and strategy games. However, many AI applications go unnoticed as AI because they integrate into common usage. Traditional AI research focuses on reasoning, knowledge representation, planning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ** text Translation**"
      ],
      "metadata": {
        "id": "FprCevCY9IOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(input_text, target_language):\n",
        "    # Use the client to create a chat completion for translation\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Translate the following text to {target_language}:\\n{input_text}\"}\n",
        "        ],\n",
        "        max_tokens=150  # Adjust max_tokens as needed\n",
        "    )\n",
        "    # Use the model's attributes to access the content\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Example input text for translation\n",
        "input_text = \"Homo sapiens are the modern man\"\n",
        "target_language = \"Spanish\"  # Specify the target language\n",
        "\n",
        "# Translate the input text\n",
        "translation = translate_text(input_text, target_language)\n",
        "print(f\"Translation to {target_language}:\\n\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKE6m_8dANXI",
        "outputId": "083c5a2a-2560-4ef2-c8b2-8ea85ac06e66"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation to Spanish:\n",
            " Los Homo sapiens son el hombre moderno.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Audio process with chat-gpt-4**"
      ],
      "metadata": {
        "id": "T9I2eP6xBCXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EJkKiDXLG_fI",
        "outputId": "bd9767d7-9d3b-4cbe-ce25-e9a82ac573c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Audio Processing with GPT-4 (Whisper)**"
      ],
      "metadata": {
        "id": "q28HUFGoHPTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "import whisper\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the audio file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe audio file\n",
        "audio_file_path = next(iter(uploaded.keys()))\n",
        "transcription = model.transcribe(audio_file_path)\n",
        "print(\"Transcription:\\n\", transcription['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "E-r6V69FMvT_",
        "outputId": "31b597ac-811a-481e-bd71-24792a9dbd93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6f50d727-4027-47d8-9178-d7adfd7eaecc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6f50d727-4027-47d8-9178-d7adfd7eaecc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving A Mysterious Design That Appears Across Millennia  Terry Moore  TED.m4a to A Mysterious Design That Appears Across Millennia  Terry Moore  TED.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:04<00:00, 33.5MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:\n",
            "  This is Roger Penrose, certainly one of the great scientists of our time. Winner of the 2020 Nobel Prize in Physics for his work reconciling black holes with Einstein's general theory of relativity. But back in the 1970s, Roger Penrose made a contribution to the world of mathematics and that part of mathematics known as tiling. You know, tiling the process of putting tiles together so that they form a particular pattern. The thing that was remarkable about the pattern that Roger Penrose developed is that by using only two shapes, he constructed a pattern that could be expanded infinitely in any direction without ever repeating. Much like the number pi has a decimal that isn't random, but it will go on forever without repeating. In mathematics, this is a property known as a periodicity. And the notion of an a periodic tile set using only two tiles was such a sensation that was given the name Penrose tiling. Here's Roger Penrose, now Sir Roger Penrose standing on a field of Penrose tiles. Then in 2007, this man, Peter Lew, who was then a graduate student in Physics at Princeton, while on vacation with his cousin in Uzbekistan, discovered this pattern on a 14th century madrasa. And after some analysis concluded that this was in fact Penrose tiling 500 years before Penrose. That information took the scientific world by storm and prompted headlines everywhere, including Discover Magazine, which proclaimed this the 59 most important scientific discovery of the year 2007. So now we've heard about this amazing pattern from the point of view of mathematics and from physics. And now art and archaeology, so that leads us to the question, what was there about this pattern? That this ancient culture found so important that they put it on their most important building. So for that, we look to the world of anthropology and ask the question, what was the world view of the culture that made this? And this is what we learned. This pattern is life. And as you can see, life's complicated. It's complicated. But not only is life complicated, life is also aparyotic in the sense that every event, every happening, every decision will make the future unfold differently, often in ways that are impossible to predict. Yet in spite of the complexity and in spite of a future that's impossible to predict, there remains an underlying unity that holds everything together and gives rise to everything. Let's see how that works in a design much like the one Peter Lew found in the spec of Stan. This is that design. Now it turns out this is actually based on this set of Penrose tiles, which are reducible to these shapes. And in order to draw these shapes, the medieval craftsmen who did this would have done them by using these construction lines. And I add here that the construction lines don't appear in the final work. But if we add them back, we have this. And now if we weave them together, we will have this. And now if we hide the tiles and just look at the construction lines, we see this. Clearly, there's an underlying structure and unity to things that seem to be complex and apiatic. This notion of a hidden underlying unity was common throughout the ancient world. And one sees it in Egypt, in Greece, in Australia, in Mesoamerica, in North America, in Europe, in the Middle East. Now in the modern West, we might call this underlying unity God, but throughout the ages, other terms have been used to describe the same thing. This is what Plato called first cause. In the medieval period, philosopher Spinoza called this the singular substance. In the 20th century, a number of terms were coined to describe this. One of my favorites being from philosopher Alfred North Whitehead, who called this the undifferentiated aesthetic continuum. Doesn't that have a 20th century sound to it? But for me, a lover of science that I am, I will take the term coined by the great 20th century physicist David Bohm, who called this the implicate order. So what's the takeaway here? Very simply this. And we see these wonderful designs created by cultures that are separated from our own, by thousands of miles or thousands of years. We can't know these are decorations. These are statements about the fundamental values that culture had, what they found important how they saw themselves, the world and themselves in the world. It has been said that architecture is a book written in stone. So when we see these amazing designs, we can know they're not decorations. They're a statement. They're a message. Look, listen. You can hear their voices. Thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summarization**"
      ],
      "metadata": {
        "id": "ALixgdvuPRIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the transcribed text using GPT-4\n",
        "def summarize_audio_text(transcribed_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n{transcribed_text}\"}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "summary = summarize_audio_text(transcription['text'])\n",
        "print(\"Summary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhS8YuR6PbXf",
        "outputId": "faaef435-bbdb-4c46-bc9b-90a17b8f6f6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " Roger Penrose, a notable scientist and the 2020 Nobel Prize in Physics laureate, made significant contributions to mathematics, specifically in tiling, forming unique patterns with two shapes that can expand infinitely without repetition. His concept, famously known as \"Penrose tiling,\" was surprisingly found on a 14th-century building in Uzbekistan by Peter Lew, a physics graduate student, in 2007, marking the structure as a phenomenon that predates Penrose's invention. The discovery led to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Processing with Gemini**"
      ],
      "metadata": {
        "id": "1ZmQOQBaPr7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BsIS-sVqHMKy"
      }
    }
  ]
}